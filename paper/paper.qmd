<!-- Should be 20 to 25 pages, extensive citing (in every section) -->

# Introduction

## Motivation

In the era of cloud computing, outsourcing data storage has become a standard practice for individuals and organizations alike. It offers benefits such as scalability, cost-efficiency, and high availability. However, storing sensitive data on third-party servers raises significant privacy and security concerns. If the cloud provider is compromised or untrustworthy, sensitive information could be exposed.

To mitigate these risks, encryption is the standard defense. Encrypting data before uploading it ensures that the cloud provider, or any adversary who gains access to the storage, cannot read the plaintext information. However, standard encryption destroys the ability to efficiently search the data. Trivial solutions, such as downloading the entire dataset and decrypting it locally to perform a search, negate the benefits of cloud storage (scalability and bandwidth efficiency). This creates a fundamental tension between data security and data utility @Poh2017SearchableSE.

Searchable Symmetric Encryption (SSE) addresses this problem by allowing a client to store encrypted data on a server in such a way that the client can later execute search queries (e.g., keyword searches) and retrieve the matching documents without revealing the plaintext data or the query keywords to the server. This technology is crucial for enabling secure cloud storage services where privacy is paramount, such as in medical records systems, legal archives, and personal data backups @Bsch2014ASO.

## Problem Statement

The core problem addressed in this thesis is the design and implementation of a Searchable Symmetric Encryption (SSE) prototype that balances efficiency, functionality, and security. While basic SSE schemes for single-keyword search exist, real-world applications often require more complex query capabilities, such as boolean queries (conjunctive searches) and substring searches (finding patterns within words) @Faber2015RichQO.

Implementing these advanced features securely is non-trivial. It requires sophisticated cryptographic primitives and careful data structure design to minimize "leakage"-information that the server can infer about the data or queries from the access patterns. Furthermore, the implementation must be practical, scaling reasonably well with the size of the database. This project aims to demonstrate a functional SSE system in .NET that supports single-keyword, boolean, and substring queries, providing a tangible exploration of these theoretical concepts.

## Outline

This thesis is structured as follows:

- **Chapter 2 (Method)** lays the theoretical foundation, explaining the definitions of SSE, the security models, and the specific cryptographic constructions used for keyword, boolean, and substring search.
- **Chapter 3 (Implementation)** details the software architecture of the developed prototype, describing the project structure, key components, and providing code snippets to illustrate the core logic.
- **Chapter 4 (Users Guide)** provides a practical guide on how to install, configure, and use the Command Line Interface (CLI) application developed in this project.
- **Chapter 5 (Alternative Methods)** discusses other approaches to searching on encrypted data, such as Deterministic Encryption and Fully Homomorphic Encryption, comparing them to SSE.
- **Chapter 6 (Discussion)** concludes the thesis, summarizing the achievements, analyzing the limitations of the current prototype, and suggesting directions for future work.

# Method

## Theoretical Foundation

Searchable Symmetric Encryption (SSE) allows a client to encrypt a document collection $D$ and outsource it to an honest-but-curious server. The server should be able to identify documents matching a query $q$ generated by the client, without learning information about the plaintext $D$ or $q$, beyond well-defined leakage @Curtmola2006SearchableSE.

An honest-but-curious server is a standard adversary model in cryptographic protocols. In this model, the server is assumed to faithfully execute the protocol specifications (it does not modify data, drop messages, or deviate from the algorithm) but will analyze all the information it receives (ciphertexts, tokens, access patterns) to infer as much information as possible about the underlying data. This contrasts with a malicious adversary, who might actively tamper with the protocol to extract secrets or disrupt the service.

An SSE scheme generally consists of four algorithms: `KeyGen`, `BuildIndex`, `Trapdoor`, and `Search`.

1.  `KeyGen`: Generates a secret key $K$.
2.  `BuildIndex`: Takes the database $D$ and key $K$ to produce an encrypted index $I$ and a set of encrypted documents $C$.
3.  `Trapdoor`: Takes the key $K$ and a keyword $w$ to generate a search token $t_w$.
4.  `Search`: Takes the index $I$ and token $t_w$ to return the identifiers of documents containing $w$.

```
%%| label: fig-sse-sequence
%%| fig-cap: "Sequence Diagram of the SSE Protocol"
sequenceDiagram
    participant Client
    participant Server


    Note over Client, Server: Setup

    Client->>Client: Generate Key (K)
    Client->>Client: Build Index (I) & Ciphertexts (C)
    Client->>Server: Upload Index (I) and Encrypted Docs (C)
        
    Note over Client, Server: Search
    Client->>Client: User inputs query keyword (w)
    Client->>Client: Generate Token (t_w) using K
    Client->>Server: Send Search Token (t_w)
    

    Server->>Server: Query Index (I) with Token (t_w)
    Server-->>Client: Return Encrypted Identifiers
    
    Client->>Client: Decrypt Results
```

### Basic Keyword Search (Inverted Index)

The foundation of efficient SSE is the inverted index. To appreciate its efficiency, it is useful to contrast it with a forward index. A forward index maps each document to the list of keywords it contains. To find all documents containing a specific keyword using a forward index, the system must scan every document's keyword list. This results in a search complexity linear to the total size of the database ($O(N)$), which is inefficient for large datasets.

In contrast, a plaintext inverted index maps each keyword directly to a list of document identifiers. This allows for near-instant retrieval ($O(r)$, where $r$ is the number of results) regardless of the total database size. In SSE, this structure is encrypted. The scheme implemented in this project follows the "SSE-1" construction by @Curtmola2006SearchableSE and the improvements by @Cash2013HighlyScalableSS.

Instead of mapping a keyword $w$ directly to a list, we generate a label for the list using a Pseudo-Random Function (PRF): $label = F_K(w)$. The list of document identifiers is encrypted. To hide the size of the lists and the access pattern to some extent, the identifiers are often encrypted individually and stored in a dictionary mapping random labels to encrypted identifiers.

### Boolean Queries (OXT Protocol)

For conjunctive queries (e.g., "search for documents containing $w_1 \land w_2 \dots \land w_n$"), simply intersecting the result sets of single-keyword searches leaks the document lists for all keywords. To avoid this, our prototype implements the **Oblivious Cross-Tags (OXT)** protocol proposed by @Cash2013HighlyScalableSS.

OXT minimizes leakage by selecting the least frequent keyword as the "pivot" ($w_1$). The server retrieves candidate documents for $w_1$ and then obliviously checks if these candidates also contain the other keywords ($w_2, \dots, w_n$) using a precomputed index called the XSet.

The scheme relies on computations in a cyclic group $\mathbb{G}$ of prime order $p$ with generator $g$. We use integer modulation where $\mathbb{G} = \mathbb{Z}_p^*$ (specifically a large prime $p$ as defined in NIST curves, though we operate in the integer field).

#### Data Structures

The server stores two main data structures:

1.  **TSet (Tuple Set):** Contains the inverted list for the pivot keyword. For a keyword $w$ and its $c$-th matching document with identifier $id$, the TSet stores a tuple:
    $$ T[w, c] = \langle e, y \rangle $$
    where:
    - $e \leftarrow \text{Enc}(K_e, id)$ is the encrypted document identifier.
    - $y \leftarrow xid \cdot z_c^{-1} \pmod{p-1}$.
    
    Here, $xid$ is a semi-static blind for the document ID derived as $xid = F_p(K_I, id)$, and $z_c$ is a dynamic blinding factor specific to the keyword instance, derived as $z_c = F_p(K_Z, w || c)$.

2.  **XSet (Cross-Tag Set):** Contains membership tags used to filter candidates. For every keyword $w$ appearing in document $id$, we store:
    $$ XSet \ni g^{xtag(w) \cdot xid} \pmod p $$
    where $xtag(w) = F_p(K_X, w)$ is a blinded tag for the keyword.

#### Search Protocol

To perform a conjunctive search for $w_1 \land w_2$:

1.  **Token Generation:** The client sends a search token for the pivot $w_1$ to the server. The server retrieves the $r$ tuples from the TSet.
2.  **XToken Calculation:** For each retrieved tuple (indexed by $c \in [1, r]$), the client computes a "cross-token" (xtoken) for the secondary keyword $w_2$:
    $$ xtoken[c, 2] = g^{z_c \cdot xtag(w_2)} \pmod p $$
3.  **Server Check:** The server validates if the candidate document contains $w_2$ by computing:
    $$ (xtoken[c, 2])^y \pmod p $$
    Substituting the values reveals:
    $$ (g^{z_c \cdot xtag(w_2)})^{xid \cdot z_c^{-1}} = g^{xtag(w_2) \cdot xid} \pmod p $$
    
    The server checks if this resulting value exists in the **XSet**. If it does, the intersection is valid, and the server returns the encrypted identifier $e$.

This protocol ensures the server learns the number of documents matching the pivot $w_1$ and which of those also match $w_2$, but it never learns the total number of documents containing $w_2$ (hiding the non-pivot result set size).

### Substring Search

Substring search (finding "net" in "internet") is achieved by decomposing words into $q$-grams (substrings of length $q$). For example, "apple" with $q=3$ yields "app", "ppl", "ple". A naive approach is to index all $q$-grams. A search for a pattern is then a search for the conjunction of its constituent $q$-grams.

However, position matters. For example, "act" and "cat" have the same letters but different meanings. To handle this, @Faber2015RichQO proposed including position information or adjacency information in the index. Our implementation uses a variation of this, indexing $q$-grams and "adjacency grams" (e.g., "app|ppl") to ensure the $q$-grams appear contiguously in the source text.

# Implementation

The prototype is implemented in **C#** using **.NET 8**, leveraging its strong typing and modern language features. The solution is organized into several projects to separate concerns: `SSE.Core`, `SSE.Client`, `SSE.Server`, `SSE.Cryptography`, and `SSE.CLI`.

## Project Structure

-   **SSE.Core**: Contains shared models (e.g., `Database`, `EncryptedDatabase`) and interfaces (`IIndex`).
-   **SSE.Cryptography**: Provides cryptographic primitives. It wraps .NET's `AES` and `HMACSHA256` and implements modular arithmetic using `System.Numerics.BigInteger` for the OXT protocol.
-   **SSE.Client**: Implements the logic for the SSE schemes. This is where the index construction (`Setup`) and query token generation (`Search`) happen.
-   **SSE.Server**: Simulates the cloud server. It holds the encrypted database and executes the search logic using tokens provided by the client.
-   **SSE.CLI**: A Text-based User Interface (TUI) for interacting with the system.

## Key Components

### Basic Scheme

The `BasicScheme` class implements the single-keyword search. The `Setup` method iterates through the keyword metadata. For each keyword, it derives two keys: a `labelKey` (to locate the list) and an `identifierKey` (to encrypt the document IDs).

```csharp
public static (byte[], EncryptedDatabase) Setup(Database<(string, string)> db)
{
    byte[] masterKey = CryptoUtils.GenerateRandomKey();
    var result = new List<(byte[] label, byte[] identifier)>();
    var metadata = db.GetMetadata();

    foreach (var keyword in metadata.Keys)
    {
        var labelKey = GetLabelKey(masterKey, keyword);
        var identifierKey = GetIdentifierKey(masterKey, keyword);
        // ... Encrypt identifiers and add to result ...
    }
    // ... Sort and build dictionary ...
}
```

### Boolean Query Scheme (OXT)

The `BooleanQueryScheme` is more complex. It implements the OXT protocol. The `Setup` phase builds the `TSet` (posting lists) and the `XSet` (membership tags).

The XSet tags are constructed using modular exponentiation in a prime group. The `BuildConjunctiveMembershipTag` method illustrates this:

```csharp
private BigInteger BuildConjunctiveMembershipTag(string keyword, BigInteger crossIdentifierExp)
{
    // ... Randomize keyword ...
    var exponent = CryptoUtils.ModMultiply(obscuredKeyword, crossIdentifierExp, ExponentGroupOrder);
    return BigInteger.ModPow(
        CryptoUtils.Generator(PrimeModulus),
        exponent,
        PrimeModulus
    );
}
```

This ensures that the server can verify membership only if it possesses the correct trapdoor information provided during the search protocol.

### Substring Query Scheme

The `SubstringQueryScheme` builds upon the `BooleanQueryScheme`. It preprocesses the text by extracting $q$-grams. It pads the text with boundary characters (e.g., `#`) to distinguish prefixes and suffixes.

```csharp
private static IEnumerable<string> ExtractFixedQGrams(string text, int q)
{
    if (string.IsNullOrEmpty(text) || text.Length < q) yield break;
    for (int i = 0; i + q <= text.Length; i++) 
        yield return text.Substring(i, q);
}
```

The search logic decomposes the query pattern into $q$-grams and executes a boolean AND query on the underlying OXT scheme.

# Users Guide

The prototype includes a Console-based application (`SSE.CLI`) that demonstrates the functionality of the implemented schemes.

## Getting Started

1.  **Prerequisites**: Ensure the .NET 8 SDK is installed.
2.  **Build**: Run `dotnet build` in the solution root.
3.  **Run**: Execute `dotnet run --project SSE.CLI`.

## Using the Interface

Upon launching, the main menu is displayed:

```text
================================================================================
 SSE Prototype - Select a Scheme
================================================================================
1. Basic Scheme (Cash et al. 2013)
2. Boolean Query Scheme (OXT - Cash et al. 2013)
3. Substring Query Scheme (Faber et al.)
4. Exit

Select an option:
```

### 1. Selecting a Scheme

Press `1`, `2`, or `3` to select the desired SSE scheme.

### 2. Loading Data

After selecting a scheme, you will be prompted to choose a data source:

-   **Use Sample Data**: Loads a small, hardcoded set of documents for quick testing.
-   **Load from Directory**: Allows you to specify an absolute path to a folder containing `.txt` files. Each file is treated as a document.

### 3. Performing Searches

Once the data is loaded and encrypted (Setup phase), you can perform searches.

-   **Basic Scheme**: Enter a single keyword (e.g., "encryption").
-   **Boolean Scheme**: Enter multiple keywords separated by spaces (e.g., "encryption security"). This will find documents containing *both* terms.
-   **Substring Scheme**: Enter a partial string (e.g., "crypt"). The system will find documents containing this fragment (e.g., "decryption", "cryptography").

# Experimental Results

To evaluate the performance of the implemented SSE schemes, we conducted a series of benchmarks. The experiments were executed on a machine equipped with an Intel Core i7-1165G7 CPU and 16 GB of RAM. The benchmarks measure the initialization time (Setup) and the search latency (Search) across the three implemented schemes: Basic, Boolean (OXT), and Substring.

## Dataset

We utilized the **20 Newsgroups** dataset (specifically the `20news-18828` version), a standard benchmark in text processing and information retrieval. Given the nature of this project, we focused on the `sci.crypt` category, which contains discussion posts about cryptography.

For the scalability experiments, we constructed subsets of increasing size from all collections, ranging from 10 up to 10000 documents. This real-world text data provides a realistic distribution of keyword frequencies and document lengths, offering a more accurate assessment of performance than synthetic random data.

## Basic Scheme Performance

The Basic Scheme (inverted index) was tested with varying dataset sizes ($10$, $100$, $1 000$ and $10000$ documents).

```{python}
#| label: fig-basic-performance
#| fig-cap: "Basic Scheme: Setup and Search Performance vs. Document Count"
#| echo: false

import pandas as pd
import matplotlib.pyplot as plt
import re

def parse_time(time_str):
    if not isinstance(time_str, str): return 0.0
    # Remove commas
    clean = time_str.replace(',', '')
    match = re.match(r'([\d\.]+)\s*(\w+)', clean)
    if not match: return 0.0
    val = float(match.group(1))
    unit = match.group(2)
    if unit == 'ns': return val / 1e6 # to ms
    if unit == 'Î¼s' or unit == 'us': return val / 1e3 # to ms
    if unit == 'ms': return val
    if unit == 's': return val * 1e3
    return val

# Load data
df = pd.read_csv("benchmark_results/SSE.Benchmark.Benchmarks.BasicSchemeBenchmarks-report.csv")

# Filter Setup and Search
setup_df = df[df['Method'] == 'SchemeSetup'].copy()
search_df = df[df['Method'] == 'Search'].copy()

# Parse times to ms
setup_df['Mean_ms'] = setup_df['Mean'].apply(parse_time)
search_df['Mean_ms'] = search_df['Mean'].apply(parse_time)

# Group by DocumentCount
setup_agg = setup_df.groupby('DocumentCount')['Mean_ms'].mean().sort_index()
search_agg = search_df.groupby('DocumentCount')['Mean_ms'].mean().sort_index()

# Plot
fig, ax = plt.subplots()

ax.plot(setup_agg.index, setup_agg.values, color='black', marker='o', label='Setup')
ax.plot(search_agg.index, search_agg.values, color='black', marker='s', linestyle='--', label='Search')

ax.set_xlabel('Document Count')
ax.set_ylabel('Time (ms)')
ax.set_yscale('log')
ax.set_xscale('log')

ax.grid(True, which="both", ls="-", alpha=0.3)
ax.legend(loc='upper left')

plt.title('Basic Scheme Scalability')
plt.show()
```

As shown in @fig-basic-performance, the setup time increases linearly with the number of documents. This behavior is expected because the `Setup` algorithm must process every word in every document, generating a pseudo-random label and encrypting the document identifier. The slope of this line represents the throughput of the index construction.

Crucially, the search time remains extremely low (in the microsecond range), though it exhibits a linear rise relative to the total database size. This observation is consistent with the properties of an inverted index on natural language data: as the document count ($N$) increases, the number of documents containing a specific common keyword ($r$) also grows linearly. Since the search complexity is $O(r)$ (proportional to the number of results found), the total search time increases with the result set size. This demonstrates that the server does not need to scan the entire database ($O(N)$) but only processes the matching entries, validating that the SSE-1 scheme maintains the efficiency of plaintext search while providing confidentiality.

## Boolean Scheme Performance

The Boolean Query Scheme (OXT) introduces additional cryptographic overhead to support conjunctive queries securely. We compared the search latency for different query complexities.

```{python}
#| label: fig-boolean-performance
#| fig-cap: "Boolean Scheme: Search Latency by Query"
#| echo: false

df_bool = pd.read_csv("benchmark_results/SSE.Benchmark.Benchmarks.BooleanSchemeBenchmarks-report.csv")
# Filter for Search method and DocumentCount of 10000
search_bool = df_bool[(df_bool['Method'] == 'Search') & (df_bool['DocumentCount'] == 10000)].copy()
search_bool['Mean_ms'] = search_bool['Mean'].apply(parse_time)

# Sort by time
search_bool = search_bool.sort_values('Mean_ms')

plt.figure()
bars = plt.barh(search_bool['searchTerms'], search_bool['Mean_ms'], color='white', edgecolor='black', hatch='///')
plt.xlabel('Search Time (ms)')
plt.title('Boolean Search Latency (10,000 Documents)')
plt.xscale('log') 
plt.grid(True, axis='x', alpha=0.3)

# Add labels
max_val = search_bool['Mean_ms'].max()
# Define a background box (bbox) with a border for better readability
bbox_props = dict(boxstyle="round,pad=0.3", fc="white", ec="black", lw=1.0, alpha=1.0)

for i, v in enumerate(search_bool['Mean_ms']):
    if v > max_val / 10:
        plt.text(v * 0.8, i, f"{v/1000:.1f} s", va='center', ha='right', fontsize=8, color='black', bbox=bbox_props)
    else:
        plt.text(v * 1.1, i, f"{v:.3f} ms", va='center', ha='left', fontsize=8)

plt.show()
```

@fig-boolean-performance illustrates the performance cost associated with the OXT protocol. Single keyword searches remain fast, but the latency increases significantly for conjunctive queries (e.g., "john crypto city").

This increase is due to the heavy cryptographic operations required for intersection. For every candidate document retrieved by the first term (the pivot), the server must perform a modular exponentiation to check the "cross-tags" for the other terms. This is a CPU-intensive operation compared to the simple XOR or hashing used in the basic scheme. The results imply that while OXT provides strong security (hiding the result sets of individual non-pivot terms), it demands more computational power on the server side, particularly for queries that return many intermediate candidates.

To better understand how the OXT protocol handles larger datasets, we also plotted the setup and search times against the document count.

```{python}
#| label: fig-boolean-scalability
#| fig-cap: "Boolean Scheme: Scalability vs. Document Count"
#| echo: false

df_bool_scale = pd.read_csv("benchmark_results/SSE.Benchmark.Benchmarks.BooleanSchemeBenchmarks-report.csv")
setup_bool_scale = df_bool_scale[df_bool_scale['Method'] == 'SchemeSetup'].copy()
search_bool_scale = df_bool_scale[df_bool_scale['Method'] == 'Search'].copy()

setup_bool_scale['Mean_ms'] = setup_bool_scale['Mean'].apply(parse_time)
search_bool_scale['Mean_ms'] = search_bool_scale['Mean'].apply(parse_time)

setup_agg_scale = setup_bool_scale.groupby('DocumentCount')['Mean_ms'].mean().sort_index()
search_agg_scale = search_bool_scale.groupby('DocumentCount')['Mean_ms'].mean().sort_index()

fig, ax = plt.subplots()

ax.plot(setup_agg_scale.index, setup_agg_scale.values, color='black', marker='o', label='Setup')
ax.plot(search_agg_scale.index, search_agg_scale.values, color='black', marker='s', linestyle='--', label='Avg Search')

ax.set_xlabel('Document Count')
ax.set_ylabel('Time (ms)')
ax.set_yscale('log')
ax.set_xscale('log')

ax.grid(True, which="both", ls="-", alpha=0.3)
ax.legend(loc='upper left')

plt.title('Boolean Scheme Scalability')
plt.show()
```

@fig-boolean-scalability shows that while setup time scales linearly (similar to the basic scheme, though with a steeper slope due to modular exponentiation), the average search time also increases with the dataset size. This is because the OXT implementation dynamically generates a "cross-token" for every candidate document matching the pivot keyword. In the 20 Newsgroups dataset, keyword frequency scales linearly with the total document count. Consequently, the client must perform a number of modular exponentiations proportional to this frequency to generate the required query tokens. This confirms that the OXT protocol's computational overhead is largely determined by the size of the intermediate result sets, validating the $O(r)$ complexity.

## Substring Scheme Performance

The Substring Scheme relies on decomposing text into $q$-grams. The choice of $q$ significantly impacts the index size and performance. We tested with $q=3, 4, 5$.

```{python}
#| label: fig-substring-performance
#| fig-cap: "Substring Scheme: Impact of q-gram size"
#| echo: false

df_sub = pd.read_csv("benchmark_results/SSE.Benchmark.Benchmarks.SubstringSchemeBenchmarks-report.csv")

# Setup
setup_sub = df_sub[df_sub['Method'] == 'Substring_Setup'].copy()
setup_sub['Mean_s'] = setup_sub['Mean'].apply(lambda x: parse_time(x) / 1000) # to seconds

# Search (Average across all terms for each q)
search_sub = df_sub[df_sub['Method'] == 'Substring_Search'].copy()
search_sub['Mean_ms'] = search_sub['Mean'].apply(parse_time)

setup_agg = setup_sub.groupby('QGramSize')['Mean_s'].mean().sort_index()
search_agg = search_sub.groupby('QGramSize')['Mean_ms'].mean().sort_index()

fig, ax1 = plt.subplots()

color = 'black'
ax1.set_xlabel('Q-Gram Size (q)')
ax1.set_ylabel('Setup Time (s)', color=color)
ax1.plot(setup_agg.index, setup_agg.values, color=color, marker='o', label='Setup')
ax1.tick_params(axis='y', labelcolor=color)
ax1.grid(True, alpha=0.3)

ax2 = ax1.twinx()
color = 'black'
ax2.set_ylabel('Avg Search Time (ms)', color=color)
ax2.plot(search_agg.index, search_agg.values, color=color, marker='^', linestyle='--', label='Search')
ax2.tick_params(axis='y', labelcolor=color)

plt.title('Impact of Q-Gram Size')
plt.xticks(setup_agg.index)
plt.show()
```

Increasing $q$ (the length of the substring) generally increases the setup time and index size because the number of unique $q$-grams grows. However, @fig-substring-performance reveals an interesting trade-off for search performance.

Larger $q$ values can actually *improve* search performance for longer patterns. This is because larger $q$-grams are more selective. A search for "encryption" using 3-grams involves intersecting many common fragments ("enc", "ncr", "cry"...), potentially processing many false positives. Using 5-grams ("encry", "ncryp"...) filters the documents much more aggressively, reducing the number of intersection checks the OXT protocol must perform. This suggests that for this dataset, selecting a moderate $q$ (like 4 or 5) strikes a balance between keeping the index size manageable and ensuring fast query response times.

# Alternative Methods

While SSE is a powerful tool, it is not the only approach to searching on encrypted data. Here we verify 2-3 alternative methods.

## Deterministic Encryption (DTE)

Deterministic Encryption (DTE) is a scheme where encrypting the same plaintext $m$ with the same key $k$ always yields the same ciphertext $c$. This contrasts with probabilistic encryption (like AES-CBC with a random IV), where the ciphertext changes every time.

**Advantages:**

-   **Efficiency:** Search is extremely fast. To search for a keyword $w$, the client simply encrypts $w$ to get $c_w$ and the server performs a standard lookup for $c_w$ in the database.
-   **Simplicity:** It can be implemented using standard database technologies (e.g., SQL equality checks).

**Disadvantages:**

-   **Security:** DTE leaks the **frequency** of keywords (if "apple" appears 10 times, the same ciphertext appears 10 times) and the **repetition pattern**. This makes it vulnerable to frequency analysis attacks, especially on datasets with low entropy (e.g., gender, age) @Bsch2014ASO. It provides a much lower security guarantee than SSE.

## Fully Homomorphic Encryption (FHE)

Fully Homomorphic Encryption (FHE) allows arbitrary computations to be performed on ciphertext without decrypting it. If $Enc(x)$ is the encryption of $x$, FHE allows computing $Enc(f(x))$ given only $Enc(x)$ and the function $f$.

**Advantages:**

-   **Security:** It is the "holy grail" of cryptography. It leaks almost nothing to the server, as the server blindly computes functions without seeing data or access patterns.
-   **Flexibility:** It supports any type of query, not just search.

**Disadvantages:**

-   **Performance:** Despite recent advances, FHE remains computationally prohibitive for large-scale search operations. It is orders of magnitude slower than SSE @Poh2017SearchableSE. Searching a database essentially requires the server to "touch" every encrypted record to compute the result, preventing the use of efficient indices.

## Oblivious RAM (ORAM)

Oblivious RAM (ORAM) is a cryptographic primitive that hides the **access pattern** of a client accessing memory. Even if the data is encrypted, the sequence of memory addresses accessed can leak information. ORAM continuously shuffles and re-encrypts data as it is accessed to obfuscate these patterns.

**Advantages:**

-   **Access Pattern Security:** It provides stronger security than standard SSE by hiding which documents are being accessed.

**Disadvantages:**

-   **Overhead:** ORAM introduces significant bandwidth and computational overhead. Each logical read/write operation translates to multiple physical read/writes (often logarithmic or polylogarithmic in the dataset size). While more efficient than FHE, it is generally slower than optimized SSE schemes for simple search tasks @Curtmola2006SearchableSE.

# Discussion

## Conclusion

This thesis presented the design and implementation of a Searchable Symmetric Encryption (SSE) prototype in .NET. We successfully implemented three distinct schemes: a basic single-keyword scheme, a boolean query scheme based on the OXT protocol, and a substring query scheme using $q$-grams. The prototype demonstrates that it is possible to perform complex searches on encrypted data without compromising the confidentiality of the documents. The accompanying CLI provides a user-friendly way to interact with these complex cryptographic constructs.

## Limitations

Despite the successful implementation, the current prototype has several limitations:

1.  **Static Data:** The current implementation is static. Once the index is built, new documents cannot be added without rebuilding the entire index. Dynamic SSE (DSSE) schemes exist @Cash2014DynamicSE @Hahn2014SearchableEW but were outside the scope of this project.
2.  **Memory Usage:** The prototype loads the entire index into memory. For very large datasets, a disk-based implementation (e.g., using a key-value store like RocksDB or Redis) would be necessary.
3.  **Leakage:** As with all efficient SSE schemes, the prototype leaks the **search pattern** (whether the same query is repeated) and the **access pattern** (which documents match the query). While OXT minimizes leakage for boolean queries, it does not eliminate it.

## Outlook

Future work could focus on addressing these limitations. Implementing **Dynamic SSE** would allow for real-time updates to the encrypted database, making the system suitable for active file systems. Integrating a **persistent storage backend** would enable the system to scale to millions of documents. Finally, exploring **Verifiable SSE** @Bost2016VerifiableDS would ensure that a malicious server cannot return incomplete or incorrect results without detection.
