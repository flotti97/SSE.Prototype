<!-- Should be 20 to 25 pages, extensive citing (in every section) -->

# Introduction

## Motivation

The rapid adoption of cloud computing has fundamentally transformed how data is stored and processed. Organizations and individuals increasingly rely on third-party cloud service providers (CSPs) for data storage due to benefits such as cost reduction, high availability, and elastic scalability. Consequently, vast amounts of sensitive information, ranging from personal health records and financial transactions to proprietary corporate data, are now stored on infrastructure physically controlled by external entities @Bsch2014ASO.

This outsourcing of data creates a significant trust boundary. While CSPs generally implement robust perimeter security, data stored in the cloud remains vulnerable to a variety of threats. These include internal attacks by malicious administrators, external data breaches resulting from software vulnerabilities and government surveillance. If the CSP is compromised or acts in an "honest-but-curious" manner, adhering to protocols but analyzing stored data to infer private information, confidentiality is lost @Bsch2014ASO.

To counter these risks, end-to-end encryption is used to protect confidential data. Via encryption at the client side before upload, data owners ensure that the CSP sees only ciphertext. However, standard encryption algorithms (e.g., AES) destroy the structure of the data required for efficient information retrieval. A standard encrypted file is effectively an unusable array of data that cannot be checked if it contains the word "urgent" without decrypting it first @Poh2017SearchableSE.

This leads to a dilemma: how to utilize the cloud for storage while maintaining the utility of the data? The trivial solution is to download the entire encrypted dataset, decrypt it locally, and perform the search on the client device. But this approach defeats the primary purpose of cloud storage. It incurs prohibitive bandwidth costs and requires the client to possess sufficient local storage and processing power to handle the entire dataset, effectively rendering the cloud a backup drive rather than a functional storage service @Poh2017SearchableSE.

Searchable Symmetric Encryption (SSE) could be the cryptographic solution to this tension between security and utility. SSE schemes allow a client to encrypt data in a specific way that enables the server to execute search queries directly on the ciphertexts. The server returns the matching encrypted documents without ever learning the plaintext content of the documents or the keywords being searched. This capability is critical for building applications where strict confidentiality and operational efficiency are required @Curtmola2006SearchableSE. 

## Problem Statement

The core problem addressed in this thesis is the design and implementation of a Searchable Symmetric Encryption (SSE) prototype that balances efficiency, functionality, and security. While basic SSE schemes for single-keyword search exist, real-world applications often require more complex query capabilities, such as boolean queries (conjunctive searches) and substring searches (finding patterns within words) @Faber2015RichQO.

Implementing these advanced features securely is non-trivial. It requires sophisticated cryptographic primitives and careful data structure design to minimize "leakage"-information that the server can infer about the data or queries from the access patterns. To make these techniques production ready, the implementation must be practical, scaling reasonably well with the size of the database. This project aims to demonstrate a functional SSE system in .NET that supports single-keyword, boolean, and substring queries, providing a tangible exploration of these theoretical concepts.

## Outline

This thesis is structured as follows:

- **Chapter 2 (Method)** lays the theoretical foundation, explaining the definitions of SSE, the security models, and the specific cryptographic constructions used for keyword, boolean, and substring search.
- **Chapter 3 (Implementation)** details the software architecture of the developed prototype, describing the project structure, key components, and providing illustrations of the core logic.
- **Chapter 4 (Users Guide)** provides a practical guide on how to install, configure, and use the Command Line Interface (CLI) application developed in this project.
- **Chapter 5 (Alternative Methods)** discusses other approaches to searching on encrypted data, such as Deterministic Encryption and Fully Homomorphic Encryption, comparing them to SSE.
- **Chapter 6 (Discussion)** concludes the thesis, summarizing the achievements, analyzing the limitations of the current prototype, and suggesting directions for future work.

# Method

## Theoretical Foundation

Searchable Symmetric Encryption (SSE) allows a client to encrypt a document collection $D$ and outsource it to an honest-but-curious server. The server should be able to identify documents matching a query $q$ generated by the client, without learning information about the plaintext $D$ or $q$, beyond well-defined leakage @Curtmola2006SearchableSE.

An honest-but-curious server is a standard adversary model in cryptographic protocols. In this model, the server is assumed to faithfully execute the protocol specifications (it does not modify data, drop messages, or deviate from the algorithm) but will analyze all the information it receives (ciphertexts, tokens, access patterns) to infer as much information as possible about the underlying data. This contrasts with a malicious adversary, who might actively tamper with the protocol to extract secrets or disrupt the service @Bsch2014ASO.

An SSE scheme generally consists of four algorithms: `KeyGen`, `BuildIndex`, `Trapdoor`, and `Search`.

1.  `KeyGen`: Generates a secret key $K$.
2.  `BuildIndex`: Takes the database $D$ and key $K$ to produce an encrypted index $I$ and a set of encrypted documents $C$.
3.  `Trapdoor`: Takes the key $K$ and a keyword $w$ to generate a search token $t_w$.
4.  `Search`: Takes the index $I$ and token $t_w$ to return the identifiers of documents containing $w$.

![Sequence diagram of the SSE Protocol](assets/sequence_diagram.svg){width=80%}

### Leakage Definitions

To achieve sub-linear search time, SSE schemes intentionally reveal specific limited information to the server, referred to as *leakage*. This trade-off allows SSE to be practical for large datasets, differing from techniques like ORAM which maximize security at the cost of performance. The standard leakage profile typically comprises two components @Curtmola2006SearchableSE:

1.  **Search Pattern:** This reveals whether the same keyword is being searched for multiple times. Formally, for a sequence of $q$ queries, the search pattern is a symmetric matrix indicating which queries in the sequence are identical.
2.  **Access Pattern:** This reveals the results of the query. For a given query $w$, the server learns the set of identifiers for documents containing $w$.

While the plaintext data remains encrypted, this leakage allows the server to learn statistical information, such as the frequency of keywords (Volume Pattern) or the correlation between documents. The schemes implemented in this thesis target this standard leakage profile, ensuring that no information beyond the search and access patterns is revealed.

### Basic Keyword Search (SSE-1)

The foundation of efficient SSE is the inverted index. To appreciate its efficiency, it is useful to contrast it with a forward index. A forward index maps each document to the list of keywords it contains, resulting in a search complexity linear to the total size of the database ($O(N)$). In contrast, an inverted index maps each keyword directly to a list of document identifiers, allowing for retrieval proportional to the number of results ($O(r)$) @Curtmola2006SearchableSE.

The scheme implemented in this project follows the "SSE-1" construction by @Curtmola2006SearchableSE and the improvements by @Cash2013HighlyScalableSS.

#### Data Structure (TSet)

The encrypted index, referred to as the TSet, is implemented as a dictionary. For a keyword $w$ appearing in a list of documents $D_w = (id_0, id_1, \dots, id_{r-1})$, the client generates two keys derived from the master key $K$:

1.  **Label Key ($K_L$):** Used to determine the storage location (label) in the dictionary.
2.  **Identifier Key ($K_I$):** Used to encrypt the document identifier.

For each document $id_c$ at index $c$ in the list, the client computes a label $l$ and a ciphertext $e$:
$$ l \leftarrow F(K_L, c) $$
$$ e \leftarrow \text{Enc}(K_I, id_c) $$
The TSet stores the mapping $TSet[l] = e$.

#### Search Protocol

To perform a search for keyword $w$, the protocol proceeds as follows:

1.  **Token Generation:** The client computes the keys $K_L$ and $K_I$ specific to $w$ and sends them to the server as the search token.
2.  **Server Execution:**
    -   The server initializes a counter $c = 0$.
    -   It repeatedly computes $l = F(K_L, c)$ and checks if $l$ exists in the TSet.
    -   If found, it retrieves $e$, decrypts it using $K_I$ to obtain the document identifier $id_c$, and increments $c$.
    -   The process stops when a label is not found.

This protocol ensures that the leakage is limited strictly to the search pattern and the access pattern. The server learns the number of results ($r$) and which documents contain the keyword, but gains no information about the keyword's value or the contents of non-retrieved documents.

### Boolean Queries (OXT Protocol)

For conjunctive queries (e.g., "search for documents containing $w_1 \land w_2 \dots \land w_n$"), simply intersecting the result sets of single-keyword searches leaks the document lists for all keywords. To avoid this, our prototype implements the **Oblivious Cross-Tags (OXT)** protocol proposed by @Cash2013HighlyScalableSS.

OXT minimizes leakage by selecting the least frequent keyword as the "pivot" ($w_1$). The server retrieves candidate documents for $w_1$ and then obliviously checks if these candidates also contain the other keywords ($w_2, \dots, w_n$) using a precomputed index called the XSet.

The scheme relies on computations in a cyclic group $\mathbb{G}$ of prime order $p$ with generator $g$. We use integer modulation where $\mathbb{G} = \mathbb{Z}_p^*$ (specifically a large prime $p$ as defined in NIST curves, though we operate in the integer field).

#### Data Structures

The server stores two main data structures:

1.  **TSet (Tuple Set):** Contains the inverted list for the pivot keyword. For a keyword $w$ and its $c$-th matching document with identifier $id$, the TSet stores a tuple:
    $$ T[w, c] = \langle e, y \rangle $$
    where:
    - $e \leftarrow \text{Enc}(K_e, id)$ is the encrypted document identifier.
    - $y \leftarrow xid \cdot z_c^{-1} \pmod{p-1}$.
    
    Here, $xid$ is a semi-static blind for the document ID derived as $xid = F_p(K_I, id)$, and $z_c$ is a dynamic blinding factor specific to the keyword instance, derived as $z_c = F_p(K_Z, w || c)$.

2.  **XSet (Cross-Tag Set):** Contains membership tags used to filter candidates. For every keyword $w$ appearing in document $id$, we store:
    $$ XSet \ni g^{xtag(w) \cdot xid} \pmod p $$
    where $xtag(w) = F_p(K_X, w)$ is a blinded tag for the keyword.

#### Search Protocol

To perform a conjunctive search for $w_1 \land w_2$:

1.  **Token Generation:** The client sends a search token for the pivot $w_1$ to the server. The server retrieves the $r$ tuples from the TSet.
2.  **XToken Calculation:** For each retrieved tuple (indexed by $c \in [1, r]$), the client computes a "cross-token" (xtoken) for the secondary keyword $w_2$:
    $$ xtoken[c, 2] = g^{z_c \cdot xtag(w_2)} \pmod p $$
3.  **Server Check:** The server validates if the candidate document contains $w_2$ by computing:
    $$ (xtoken[c, 2])^y \pmod p $$
    Substituting the values reveals:
    $$ (g^{z_c \cdot xtag(w_2)})^{xid \cdot z_c^{-1}} = g^{xtag(w_2) \cdot xid} \pmod p $$
    
    The server checks if this resulting value exists in the **XSet**. If it does, the intersection is valid, and the server returns the encrypted identifier $e$.

**Leakage Profile:** The OXT protocol reveals the access pattern of the pivot keyword ($w_1$) and the result pattern of the intersection ($w_1 \land w_2$). Crucially, it hides the access pattern of the non-pivot keywords ($w_2$). The server learns the *conditional* probability: "Given document $D$ contains $w_1$, does it also contain $w_2$?" but does not learn if a document contains $w_2$ if it does not contain $w_1$.

### Substring Search

Substring search (finding "net" in "internet") is achieved by decomposing words into $q$-grams (substrings of length $q$). A naive approach simply indexes all $q$-grams. A search for a pattern is then treated as a boolean AND query of its constituent $q$-grams @Faber2015RichQO. However, this approach ignores the **order** and **adjacency** of the grams, leading to false positives (e.g., searching for "star" might match "rats" if only the letters are checked, or "stack overflow" might match "over flow stack").

To enforce correct ordering, we implement the method proposed by @Faber2015RichQO which indexes both individual $q$-grams and **adjacency grams**.

#### Indexing Strategy

For a given text and a fixed size $q$, we extract:

1.  **Gram Tokens ($G$):** Every substring of length $q$.
2.  **Adjacency Tokens ($A$):** Special tokens representing two specific grams appearing consecutively.

#### Query Transformation Example

Consider a search for the pattern `"test"` with $q=3$.

1.  **Decomposition:** The pattern is split into 3-grams: `"tes"`, `"est"`.
2.  **Token Generation:**
    -   $G_1 = \text{"tes"}$
    -   $G_2 = \text{"est"}$
    -   $A_1 = \text{"tes|est"}$ (representing that "tes" is immediately followed by "est")
3.  **Boolean Query:** The substring search is transformed into a conjunctive query:
    $$ \text{Query} = G_1 \land G_2 \land A_1 $$

By requiring the presence of the adjacency token $A_1$, we ensure that "tes" and "est" are not just present in the document, but present in the correct order to form "test". This logic is built directly on top of the Boolean OXT scheme described above, inheriting its security properties.

**Leakage:** Since the query is executed as a boolean conjunction of tokens, the leakage corresponds to the OXT leakage on the generated $q$-grams. The server learns the access pattern of the "pivot token" (the least frequent $q$-gram or adjacency token in the pattern) and the final result set (documents containing the full substring). The frequency of other component $q$-grams remains hidden.

While this increases the index size (adding $O(N)$ adjacency tokens), it provides precise substring search capabilities without false positives due to reordering.

# Implementation

The prototype is implemented in **C#** using **.NET 8**, a cross-platform framwork that would be suitable for production deployment. The solution is organized into several projects to separate concerns: `SSE.Core`, `SSE.Client`, `SSE.Server`, `SSE.Cryptography`, and `SSE.CLI`.

## Project Structure

-   **SSE.Core**: Contains shared models (e.g., `Database`, `EncryptedDatabase`) and interfaces (`IIndex`).
-   **SSE.Cryptography**: Provides cryptographic primitives. It wraps .NET's `AES` and `HMACSHA256` and implements modular arithmetic using `System.Numerics.BigInteger` for the OXT protocol.
-   **SSE.Client**: Implements the logic for the SSE schemes. This is where the index construction (`Setup`) and query token generation (`Search`) happen.
-   **SSE.Server**: Simulates the cloud server. It holds the encrypted database and executes the search logic using tokens provided by the client.
-   **SSE.CLI**: A Text-based User Interface (TUI) for interacting with the system.

## Key Components

### Data Structures & Serialization

A critical aspect of the implementation is the efficient storage and transmission of the encrypted index.

#### Index Storage
The encrypted index is stored in memory using .NET's `Dictionary<TKey, TValue>` class, which provides average $O(1)$ lookup time.

- **TSet:** Represented as `Dictionary<string, List<Tuple<byte[], byte[]>>>`. The key is the label $F(K, w)$, and the value is the list of tuples $\langle e, y \rangle$. The `string` key is a Base64 encoding of the label bytes to ensure compatibility with dictionary hashing.
- **XSet:** Represented as a `HashSet<BigInteger>`. We use `BigInteger` from `System.Numerics` to handle the large numbers required for cryptographic security (256-bit prime field). The `HashSet` ensures that checking for the existence of a cross-tag is an $O(1)$ operation.

#### Serialization
To simulate the network communication between the Client and the Server, we define simple Data Transfer Objects (DTOs).

- **Index Transfer:** The `EncryptedDatabase` object is serialized into a binary format (using `System.Text.Json` or custom binary writers in a real deployment; here we pass objects by reference for simulation).
- **Search Tokens:** The `QueryMessage` contains the `Stag` (byte array) and a list of `ConjunctiveTokenSets`.

#### Memory Management
The current prototype loads the entire `EncryptedDatabase` into the RAM of the server process. While efficient for the experimental dataset (20 Newsgroups, approx. 20,000 documents), this approach is memory-bound. The `Dictionary` and `HashSet` structures incur overhead for object headers and pointers. As noted in the Limitations section, a production-grade system would replace these in-memory structures with a persistent Key-Value store (e.g., Redis or RocksDB) that supports efficient point lookups.

### Cryptographic Primitives

The `SSE.Cryptography` project encapsulates all security-critical operations, ensuring consistent usage of algorithms across the client and server.

-   **PRF & MAC:** We use **HMAC-SHA256** for all pseudo-random functions ($F_K$) and key derivation.
-   **Symmetric Encryption:** Document identifiers are encrypted using **AES-256-CBC** with a random Initialization Vector (IV) prefixed to the ciphertext.
-   **Modular Arithmetic:** For the OXT protocol, we operate in the multiplicative group of integers modulo a prime $p$. We define a static 256-bit safe prime `Prime256Bit` and use `System.Numerics.BigInteger` for modular exponentiation ($g^x \pmod p$) and inversion ($x^{-1} \pmod {p-1}$).

### Basic Scheme

The `BasicScheme` class implements the single-keyword search. The `Setup` method iterates through the keyword metadata. For each keyword, it derives two keys: a `labelKey` (to locate the list) and an `identifierKey` (to encrypt the document IDs).

```csharp
public static (byte[], EncryptedDatabase) Setup(Database<(string, string)> db)
{
    byte[] masterKey = CryptoUtils.GenerateRandomKey();
    var result = new List<(byte[] label, byte[] identifier)>();
    var metadata = db.GetMetadata();

    foreach (var keyword in metadata.Keys)
    {
        // Derive keys using HMAC-SHA256
        var labelKey = GetLabelKey(masterKey, keyword);
        var identifierKey = GetIdentifierKey(masterKey, keyword);
        
        int counter = 0;
        foreach (var identifier in metadata[keyword])
        {
            // label = F(labelKey, counter)
            var randomLabel = CryptoUtils.Randomize(labelKey, counter);
            // e = Enc(identifierKey, id)
            var encryptedIdentifier = CryptoUtils.Encrypt(identifierKey, identifier);
            
            result.Add((randomLabel, encryptedIdentifier));
            counter++;
        }
    }
    // ... Sort and build dictionary ...
}
```

### Boolean Query Scheme (OXT)

The `BooleanQueryScheme` class implements the OXT protocol, mapping the theoretical constructs directly to C# objects. The `KeyCollection` struct holds the protocol keys:

-   `DocumentEncryptionKeySeed` corresponds to $K_e$.
-   `KeywordTagKey` corresponds to $K_X$ (used for $xtag$).
-   `DocumentIndexKey` corresponds to $K_I$ (used for $xid$).
-   `KeywordCounterKey` corresponds to $K_Z$ (used for $z_c$).

The `Setup` phase builds the `TSet` (posting lists) and the `XSet` (membership tags). The implementation explicitly calculates the blinding factors described in the method section.

```csharp
private (byte[] EncryptedDocId, byte[] InvertedCounterAppliedIndex) BuildPostingTuple(...)
{
    // z_c = F(K_Z, w || c)
    var occurenceFactor = DeriveKeywordOccurrenceFactor(keys.KeywordCounterKey, keyword, counter);
    // z_c^-1 mod (p-1)
    var occurenceFactorInverse = CryptoUtils.ModInverse(occurenceFactorBytes, ExponentGroupOrder);

    // y = xid * z_c^-1 mod (p-1)
    var yValue = CryptoUtils.ModMultiply(crossIdentifierExp, occurenceFactorInverse, ExponentGroupOrder);
    
    // e = Enc(K_e, id)
    var encryptedDocId = CryptoUtils.Encrypt(labelKey, identifier);
    
    return (encryptedDocId, yValue.ToByteArray(...));
}
```

For the **XSet**, the membership tags are constructed using modular exponentiation.

```csharp
private BigInteger BuildConjunctiveMembershipTag(string keyword, BigInteger crossIdentifierExp)
{
    // xtag = F(K_X, w)
    var obscuredKeyword = ...; 
    
    // exponent = xtag * xid
    var exponent = CryptoUtils.ModMultiply(obscuredKeyword, crossIdentifierExp, ExponentGroupOrder);
    
    // result = g^(xtag * xid) mod p
    return BigInteger.ModPow(
        CryptoUtils.Generator(PrimeModulus),
        exponent,
        PrimeModulus
    );
}
```

This direct mapping ensures that the implementation stays faithful to the security proofs of the OXT protocol.

### Substring Query Scheme

The `SubstringQueryScheme` builds upon the `BooleanQueryScheme`. It preprocesses the text by extracting $q$-grams and padding the text with boundary characters (specifically `#`) to distinguish prefixes and suffixes.

To enable the distinction between standard $q$-grams and adjacency constraints, we use a structured string format for tokens:

-   **Gram Tokens:** `G:{q}:{gram}` (e.g., `G:3:enc`)
-   **Adjacency Tokens:** `A:{q}:{gram1}|{gram2}` (e.g., `A:3:enc|ncr`)

The `Setup` method generates these tokens and then delegates the actual encryption to the `BooleanQueryScheme`, effectively treating the complex substring problem as a standard conjunctive keyword search.

```csharp
private static IEnumerable<string> ExtractFixedQGrams(string text, int q)
{
    if (string.IsNullOrEmpty(text) || text.Length < q) yield break;
    for (int i = 0; i + q <= text.Length; i++) 
        yield return text.Substring(i, q);
}
```

#### Query Optimization

The search logic decomposes the query pattern into $q$-grams and executes a boolean AND query on the underlying OXT scheme. Crucially, it optimizes performance by carefully selecting the order of operations.

1.  **Decomposition:** The pattern is split into its constituent $q$-grams and adjacency tokens.
    -   Example: "test" ($q=3$) $\rightarrow$ `G:3:tes`, `G:3:est`, `A:3:tes|est`.
2.  **Pivot Selection:** The OXT protocol's performance depends on the size of the result set for the first keyword (the pivot). The client consults its local metadata (the `tokenToDocIds` map) to determine the frequency of each token. It selects the token with the *fewest* matching documents as the pivot.
3.  **Execution:** The tokens are reordered so the pivot is first, and the request is sent to the OXT server.

This "selectivity estimation" ensures that the server performs the expensive cross-tag checks on the smallest possible set of candidates.

```csharp
public IEnumerable<string> Search(string pattern)
{
    // ... Decomposition into 'tokens' list ...

    // Select pivot: Token with the lowest document count
    var pivot = tokens.OrderBy(t => 
        tokenToDocIds.TryGetValue(t, out var lst) ? lst.Count : int.MaxValue
    ).First();

    // Reorder: Pivot first
    var reordered = new List<string> { pivot };
    reordered.AddRange(tokens.Where(t => t != pivot));

    // Execute Boolean Search
    return booleanScheme.Search(server, reordered.ToArray());
}
```

# Users Guide

The prototype includes a Console-based application (`SSE.CLI`) that demonstrates the functionality of the implemented schemes.

## Getting Started

1.  **Prerequisites**: Ensure the .NET 8 SDK is installed.
2.  **Build**: Run `dotnet build` in the solution root.
3.  **Run**: Execute `dotnet run --project SSE.CLI`.

## Using the Interface

Upon launching, the main menu is displayed:

```text
================================================================================
 SSE Prototype - Select a Scheme
================================================================================
1. Basic Scheme (Cash et al. 2013)
2. Boolean Query Scheme (OXT - Cash et al. 2013)
3. Substring Query Scheme (Faber et al.)
4. Exit

Select an option:
```

### 1. Selecting a Scheme

Press `1`, `2`, or `3` to select the desired SSE scheme.

### 2. Loading Data

After selecting a scheme, you will be prompted to choose a data source:

-   **Use Sample Data**: Loads a small, hardcoded set of documents for quick testing.
-   **Load from Directory**: Allows you to specify an absolute path to a folder containing `.txt` files. Each file is treated as a document.

### 3. Performing Searches

Once the data is loaded and encrypted (Setup phase), you can perform searches.

-   **Basic Scheme**: Enter a single keyword (e.g., "encryption").
-   **Boolean Scheme**: Enter multiple keywords separated by spaces (e.g., "encryption security"). This will find documents containing *both* terms.
-   **Substring Scheme**: Enter a partial string (e.g., "crypt"). The system will find documents containing this fragment (e.g., "decryption", "cryptography").

# Experimental Results

To evaluate the performance of the implemented SSE schemes, we conducted a series of benchmarks. The experiments were executed on a machine equipped with an Intel Core i7-1165G7 CPU and 16 GB of RAM. The benchmarks measure the initialization time (Setup) and the search latency (Search) across the three implemented schemes: Basic, Boolean (OXT), and Substring.

## Dataset

We utilized the **20 Newsgroups** dataset (specifically the `20news-18828` version), a standard benchmark in text processing and information retrieval. Given the nature of this project, we focused on the `sci.crypt` category, which contains discussion posts about cryptography.

For the scalability experiments, we constructed subsets of increasing size from all collections, ranging from 10 up to 10000 documents. This real-world text data provides a realistic distribution of keyword frequencies and document lengths, offering a more accurate assessment of performance than synthetic random data.

## Basic Scheme Performance

The Basic Scheme (inverted index) was tested with varying dataset sizes ($10$, $100$, $1 000$ and $10000$ documents).

```{python}
#| label: fig-basic-performance
#| fig-cap: "Basic Scheme: Setup and Search Performance vs. Document Count"
#| echo: false

import pandas as pd
import matplotlib.pyplot as plt
import re

def parse_time(time_str):
    if not isinstance(time_str, str): return 0.0
    # Remove commas
    clean = time_str.replace(',', '')
    match = re.match(r'([\d\.]+)\s*(\w+)', clean)
    if not match: return 0.0
    val = float(match.group(1))
    unit = match.group(2)
    if unit == 'ns': return val / 1e6 # to ms
    if unit == 'μs' or unit == 'us': return val / 1e3 # to ms
    if unit == 'ms': return val
    if unit == 's': return val * 1e3
    return val

# Load data
df = pd.read_csv("benchmark_results/SSE.Benchmark.Benchmarks.BasicSchemeBenchmarks-report.csv")

# Filter Setup and Search
setup_df = df[df['Method'] == 'SchemeSetup'].copy()
search_df = df[df['Method'] == 'Search'].copy()

# Parse times to ms
setup_df['Mean_ms'] = setup_df['Mean'].apply(parse_time)
search_df['Mean_ms'] = search_df['Mean'].apply(parse_time)

# Group by DocumentCount
setup_agg = setup_df.groupby('DocumentCount')['Mean_ms'].mean().sort_index()
search_agg = search_df.groupby('DocumentCount')['Mean_ms'].mean().sort_index()

# Plot
fig, ax = plt.subplots()

ax.plot(setup_agg.index, setup_agg.values, color='black', marker='o', label='Setup')
ax.plot(search_agg.index, search_agg.values, color='black', marker='s', linestyle='--', label='Search')

ax.set_xlabel('Document Count')
ax.set_ylabel('Time (ms)')
ax.set_yscale('log')
ax.set_xscale('log')

ax.grid(True, which="both", ls="-", alpha=0.3)
ax.legend(loc='upper left')

plt.title('Basic Scheme Scalability')
plt.show()
```

As shown in @fig-basic-performance, the setup time increases linearly with the number of documents. This behavior is expected because the `Setup` algorithm must process every word in every document, generating a pseudo-random label and encrypting the document identifier. The slope of this line represents the throughput of the index construction.

Crucially, the search time remains extremely low (in the microsecond range), though it exhibits a linear rise relative to the total database size. This observation is consistent with the properties of an inverted index on natural language data: as the document count ($N$) increases, the number of documents containing a specific common keyword ($r$) also grows linearly. Since the search complexity is $O(r)$ (proportional to the number of results found), the total search time increases with the result set size. This demonstrates that the server does not need to scan the entire database ($O(N)$) but only processes the matching entries, validating that the SSE-1 scheme maintains the efficiency of plaintext search while providing confidentiality.

## Boolean Scheme Performance

The Boolean Query Scheme (OXT) introduces additional cryptographic overhead to support conjunctive queries securely. We compared the search latency for different query complexities.

```{python}
#| label: fig-boolean-performance
#| fig-cap: "Boolean Scheme: Search Latency by Query"
#| echo: false

df_bool = pd.read_csv("benchmark_results/SSE.Benchmark.Benchmarks.BooleanSchemeBenchmarks-report.csv")
# Filter for Search method and DocumentCount of 10000
search_bool = df_bool[(df_bool['Method'] == 'Search') & (df_bool['DocumentCount'] == 10000)].copy()
search_bool['Mean_ms'] = search_bool['Mean'].apply(parse_time)

# Sort by time
search_bool = search_bool.sort_values('Mean_ms')

plt.figure()
bars = plt.barh(search_bool['searchTerms'], search_bool['Mean_ms'], color='white', edgecolor='black', hatch='///')
plt.xlabel('Search Time (ms)')
plt.title('Boolean Search Latency (10,000 Documents)')
plt.xscale('log') 
plt.grid(True, axis='x', alpha=0.3)

# Add labels
max_val = search_bool['Mean_ms'].max()
# Define a background box (bbox) with a border for better readability
bbox_props = dict(boxstyle="round,pad=0.3", fc="white", ec="black", lw=1.0, alpha=1.0)

for i, v in enumerate(search_bool['Mean_ms']):
    if v > max_val / 10:
        plt.text(v * 0.8, i, f"{v/1000:.1f} s", va='center', ha='right', fontsize=8, color='black', bbox=bbox_props)
    else:
        plt.text(v * 1.1, i, f"{v:.3f} ms", va='center', ha='left', fontsize=8)

plt.show()
```

@fig-boolean-performance illustrates the performance cost associated with the OXT protocol. Single keyword searches remain fast, but the latency increases significantly for conjunctive queries (e.g., "john crypto city").

This increase is due to the heavy cryptographic operations required for intersection. For every candidate document retrieved by the first term (the pivot), the server must perform a modular exponentiation to check the "cross-tags" for the other terms. This is a CPU-intensive operation compared to the simple XOR or hashing used in the basic scheme. The results imply that while OXT provides strong security (hiding the result sets of individual non-pivot terms), it demands more computational power on the server side, particularly for queries that return many intermediate candidates.

To better understand how the OXT protocol handles larger datasets, we also plotted the setup and search times against the document count.

```{python}
#| label: fig-boolean-scalability
#| fig-cap: "Boolean Scheme: Scalability vs. Document Count"
#| echo: false

df_bool_scale = pd.read_csv("benchmark_results/SSE.Benchmark.Benchmarks.BooleanSchemeBenchmarks-report.csv")
setup_bool_scale = df_bool_scale[df_bool_scale['Method'] == 'SchemeSetup'].copy()
search_bool_scale = df_bool_scale[df_bool_scale['Method'] == 'Search'].copy()

setup_bool_scale['Mean_ms'] = setup_bool_scale['Mean'].apply(parse_time)
search_bool_scale['Mean_ms'] = search_bool_scale['Mean'].apply(parse_time)

setup_agg_scale = setup_bool_scale.groupby('DocumentCount')['Mean_ms'].mean().sort_index()
search_agg_scale = search_bool_scale.groupby('DocumentCount')['Mean_ms'].mean().sort_index()

fig, ax = plt.subplots()

ax.plot(setup_agg_scale.index, setup_agg_scale.values, color='black', marker='o', label='Setup')
ax.plot(search_agg_scale.index, search_agg_scale.values, color='black', marker='s', linestyle='--', label='Avg Search')

ax.set_xlabel('Document Count')
ax.set_ylabel('Time (ms)')
ax.set_yscale('log')
ax.set_xscale('log')

ax.grid(True, which="both", ls="-", alpha=0.3)
ax.legend(loc='upper left')

plt.title('Boolean Scheme Scalability')
plt.show()
```

@fig-boolean-scalability shows that while setup time scales linearly (similar to the basic scheme, though with a steeper slope due to modular exponentiation), the average search time also increases with the dataset size. This is because the OXT implementation dynamically generates a "cross-token" for every candidate document matching the pivot keyword. In the 20 Newsgroups dataset, keyword frequency scales linearly with the total document count. Consequently, the client must perform a number of modular exponentiations proportional to this frequency to generate the required query tokens. This confirms that the OXT protocol's computational overhead is largely determined by the size of the intermediate result sets, validating the $O(r)$ complexity.

## Substring Scheme Performance

The Substring Scheme relies on decomposing text into $q$-grams. The choice of $q$ significantly impacts the index size and performance. We tested with $q=3, 4, 5$.

```{python}
#| label: fig-substring-performance
#| fig-cap: "Substring Scheme: Impact of q-gram size"
#| echo: false

df_sub = pd.read_csv("benchmark_results/SSE.Benchmark.Benchmarks.SubstringSchemeBenchmarks-report.csv")

# Setup
setup_sub = df_sub[df_sub['Method'] == 'Substring_Setup'].copy()
setup_sub['Mean_s'] = setup_sub['Mean'].apply(lambda x: parse_time(x) / 1000) # to seconds

# Search (Average across all terms for each q)
search_sub = df_sub[df_sub['Method'] == 'Substring_Search'].copy()
search_sub['Mean_ms'] = search_sub['Mean'].apply(lambda x: parse_time(x) / 1000)

setup_agg = setup_sub.groupby('QGramSize')['Mean_s'].mean().sort_index()
search_agg = search_sub.groupby('QGramSize')['Mean_ms'].mean().sort_index()

fig, ax1 = plt.subplots()

color = 'black'
ax1.set_xlabel('Q-Gram Size (q)')
ax1.set_ylabel('Setup Time (s)', color=color)
ax1.plot(setup_agg.index, setup_agg.values, color=color, marker='o', label='Setup')
ax1.tick_params(axis='y', labelcolor=color)
ax1.grid(True, alpha=0.3)

ax2 = ax1.twinx()
color = 'black'
ax2.set_ylabel('Avg Search Time (s)', color=color)
ax2.plot(search_agg.index, search_agg.values, color=color, marker='^', linestyle='--', label='Search')
ax2.tick_params(axis='y', labelcolor=color)
lines_1, labels_1 = ax1.get_legend_handles_labels()
lines_2, labels_2 = ax2.get_legend_handles_labels()
ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc='upper center')
plt.title('Impact of Q-Gram Size')
plt.xticks(setup_agg.index)
plt.show()
```

Increasing $q$ (the length of the substring) generally increases the setup time and index size because the number of unique $q$-grams grows. However, @fig-substring-performance reveals an interesting trade-off for search performance.

Larger $q$ values can actually *improve* search performance for longer patterns. This is because larger $q$-grams are more selective. A search for "encryption" using 3-grams involves intersecting many common fragments ("enc", "ncr", "cry"...), potentially processing many false positives. Using 5-grams ("encry", "ncryp"...) filters the documents much more aggressively, reducing the number of intersection checks the OXT protocol must perform. This suggests that for this dataset, selecting a moderate $q$ (like 4 or 5) strikes a balance between keeping the index size manageable and ensuring fast query response times.

### Scalability Considerations

Although we focused on the impact of $q$, the scalability of the Substring Scheme with respect to the dataset size follows the properties of the underlying OXT protocol.

1.  **Index Size & Setup Time:** These scale linearly with the total number of characters in the dataset. Unlike the word-based Basic Scheme, where the index size depends on the number of unique words, the Substring Scheme indexes every position in the text. This results in a significantly larger index (approximately $2 \times$ the size of the plaintext for $q$-grams and adjacency tokens combined), but the growth remains linear ($O(N)$).
2.  **Search Time:** Search latency is dominated by the number of documents containing the "pivot" $q$-gram. As the dataset grows, the frequency of any given $q$-gram (e.g., "ing") increases linearly. Consequently, the search time also scales linearly with the database size, similar to the Boolean Scheme. However, the absolute latency is higher due to the sheer volume of tokens processed.

# Alternative Methods

While Searchable Symmetric Encryption (SSE) strikes a balance between performance and security, it is part of a broader landscape of techniques for computing on encrypted data. To understand the design choices made in this project, it is valuable to compare SSE with two alternative cryptographic paradigms that offer different trade-offs: Fully Homomorphic Encryption (FHE) and Oblivious RAM (ORAM).

## Fully Homomorphic Encryption (FHE)

Fully Homomorphic Encryption (FHE) is often described as the "holy grail" of cryptography. It enables arbitrary computation on ciphertexts. Formally, given encryptions $E(x)$ and $E(y)$, FHE allows the computation of $E(x + y)$ and $E(x \cdot y)$ without ever decrypting the inputs. Since any computable function can be represented as a circuit of addition and multiplication gates, FHE allows a server to compute any function $f$ on encrypted data, yielding an encrypted result $E(f(x))$ @Bsch2014ASO.

**Mechanism in Search**

In the context of search, a client would send an encrypted query $E(q)$ to the server. The server would then run a boolean circuit over every encrypted document $E(d)$ in the database to evaluate whether it matches the query. The output would be an encrypted bit $E(1)$ (match) or $E(0)$ (no match), or the encrypted document itself multiplied by the match bit.

**Comparison with SSE**

The primary advantage of FHE is security. It offers the highest possible level of data privacy because the server learns absolutely nothing—not even the access patterns or search patterns. The server essentially operates "blindly" on the data.

However, this comes at a prohibitive cost in performance. Because the server cannot know which documents match, it cannot use an index (which inherently leaks access patterns). Instead, it must process every single document in the database for every query ($O(N)$ complexity). Furthermore, the cryptographic operations themselves are orders of magnitude slower than standard AES operations. This slowness is largely due to "noise" that accumulates in the ciphertext with every operation. To prevent decryption failures, this noise must be periodically reduced using an expensive procedure called *bootstrapping*. While recent years have seen significant optimizations, FHE remains impractical for searching large-scale databases compared to SSE, which enables sub-linear ($O(r)$) search times @Poh2017SearchableSE.

## Oblivious RAM (ORAM)

Oblivious RAM (ORAM) addresses a specific leakage present in SSE: the access pattern. Even if data is encrypted, the sequence of memory addresses accessed by a program can reveal information about the data itself. For example, repeatedly accessing the same location suggests the user is interested in the same item. ORAM provides a cryptographic layer that completely obfuscates these physical memory accesses @Poh2017SearchableSE.

**Mechanism**

ORAM acts as an interface between the processor (client) and the memory (server). When the client wants to access a logical block of data $A$, the ORAM protocol translates this into a sequence of read and write operations to various random-looking physical addresses. Crucially, the protocol continuously shuffles and re-encrypts data blocks as they are accessed. Accessing the same logical block twice will result in accessing completely different physical locations on the server.

Modern constructions, such as Path ORAM, organize the server's storage as a binary tree. The client maintains a local "position map" indicating which leaf node allows access to a specific data block. To read a block, the client reads the entire path from the root to that leaf, re-encrypts the path, and writes it back, potentially moving the block to a new random leaf. This ensures that every access appears as a random path traversal to the server.

**Comparison with SSE**

ORAM provides stronger security guarantees than standard SSE. By hiding the access pattern, it prevents statistical attacks that try to correlate query results with external knowledge (e.g., knowing that "encryption" is a frequent search term).

However, this security is achieved through bandwidth and computational overhead. To hide a single access, ORAM typically requires reading and writing a logarithmic or polylogarithmic number of blocks ($O(\log N)$ or $O(\log^2 N)$). While this is asymptotically better than the linear scan required by FHE ($O(N)$), it is still significantly more expensive than the constant or near-constant overhead of SSE. Consequently, ORAM is typically reserved for scenarios where access pattern leakage is considered a critical vulnerability, whereas SSE is preferred for general-purpose cloud storage systems where performance and scalability are paramount @Curtmola2006SearchableSE @Bsch2014ASO.

# Discussion

## Conclusion

This thesis presented the design and implementation of a Searchable Symmetric Encryption (SSE) prototype in .NET. We successfully implemented three distinct schemes: a basic single-keyword scheme, a boolean query scheme based on the OXT protocol, and a substring query scheme using $q$-grams. The prototype demonstrates that it is possible to perform complex searches on encrypted data without compromising the confidentiality of the documents. The accompanying CLI provides a user-friendly way to interact with these complex cryptographic constructs.

## Limitations

Despite the successful implementation, the current prototype has several limitations:

1.  **Static Data:** The current implementation is static. Once the index is built, new documents cannot be added without rebuilding the entire index. Dynamic SSE (DSSE) schemes exist @Cash2014DynamicSE @Hahn2014SearchableEW but were outside the scope of this project.
2.  **Memory Usage:** The prototype loads the entire index into memory. For very large datasets, a disk-based implementation (e.g., using a key-value store like RocksDB or Redis) would be necessary.
3.  **Leakage:** As with all efficient SSE schemes, the prototype leaks the **search pattern** (whether the same query is repeated) and the **access pattern** (which documents match the query). While OXT minimizes leakage for boolean queries, it does not eliminate it.

## Outlook

Future work could focus on addressing these limitations. Implementing **Dynamic SSE** would allow for real-time updates to the encrypted database, making the system suitable for active file systems. Integrating a **persistent storage backend** would enable the system to scale to millions of documents. Finally, exploring **Verifiable SSE** @Bost2016VerifiableDS would ensure that a malicious server cannot return incomplete or incorrect results without detection.
